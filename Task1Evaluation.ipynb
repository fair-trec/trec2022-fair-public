{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6275253",
   "metadata": {},
   "source": [
    "# Task 1 Evaluation\n",
    "\n",
    "This notebook contains the evaluation for Task 1 of the TREC Fair Ranking track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a5f0f2-8944-4082-a071-5e564e2ae9f6",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "DATA_MODE = 'eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4902afcd-449c-43a2-aa97-eb6190a5f2e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wptrec\n",
    "wptrec.DATA_MODE = DATA_MODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4c102b",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We begin by loading necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a056319",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from scipy.stats import bootstrap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import reduce\n",
    "import gzip\n",
    "import binpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4d012a-252c-4b1e-ad57-9f8b4c6fc8cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tbl_dir = Path('data/metric-tables')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705e4899",
   "metadata": {},
   "source": [
    "Set up progress bar and logging support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28734344",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas(leave=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86555293",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, logging\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stderr)\n",
    "log = logging.getLogger('task1-eval')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f190de",
   "metadata": {},
   "source": [
    "Set up the RNG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8c84ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seedbank\n",
    "seedbank.initialize(20220101)\n",
    "rng = seedbank.numpy_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971b7296",
   "metadata": {},
   "source": [
    "Import metric code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc40035",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wptrec.metrics as metrics\n",
    "from wptrec.trecdata import scan_runs, scan_teams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccf20fb",
   "metadata": {},
   "source": [
    "And finally import the metric itself.  For Task 1, this uses:\n",
    "\n",
    "* evaluation qrels\n",
    "* evaluation intersectional targets\n",
    "* all dimensions (with their page alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5742da24-089e-4fdc-afed-ecf39f2e43b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from MetricInputs import qrels, dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78234e82-5a51-4277-a962-a2a562d1f9f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target = xr.open_dataarray(tbl_dir / f'task1-{DATA_MODE}-int-targets.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c986eb-6cb0-46fc-9f61-e5a3fc4cef09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric = metrics.AWRFMetric(qrels.set_index('topic_id'), dimensions, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00c96e2",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa5969b",
   "metadata": {},
   "source": [
    "Let's load the runs now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10e9219",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "runs = pd.DataFrame.from_records(row for rows in scan_runs(1, 'runs/2022') for row in rows)\n",
    "runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8b9608-c869-4337-b393-f0ceb04511f7",
   "metadata": {},
   "source": [
    "And the teams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6234a79-db57-44e7-bc06-29cb3d16116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_runs = scan_teams('runs/2022')\n",
    "team_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87347c9-b873-4c91-9b4e-01249d833c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_team = team_runs.set_index('run')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce54b23b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Computing Metrics\n",
    "\n",
    "We are now ready to compute the metric for each (system,topic) pair.  Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc702eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rank_awrf = runs.groupby(['run_name', 'topic_id'])['page_id'].progress_apply(metric)\n",
    "rank_awrf = rank_awrf.unstack()\n",
    "rank_awrf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa65eea",
   "metadata": {},
   "source": [
    "Make sure we aren't missing anything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9903c2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rank_awrf[rank_awrf['Score'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbf43de",
   "metadata": {},
   "source": [
    "Now let's average by runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b3956b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_scores = rank_awrf.groupby('run_name').mean()\n",
    "run_scores.sort_values('Score', ascending=False, inplace=True)\n",
    "run_scores = run_scores.join(run_team)\n",
    "run_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740b95ea",
   "metadata": {},
   "source": [
    "And bootstrap some confidence intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8f7021",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def boot_ci(col):\n",
    "    res = bootstrap([col], statistic=np.mean, random_state=rng)\n",
    "    return pd.Series({\n",
    "        'Score.SE': res.standard_error,\n",
    "        'Score.Lo': res.confidence_interval.low,\n",
    "        'Score.Hi': res.confidence_interval.high,\n",
    "        'Score.W': res.confidence_interval.high - res.confidence_interval.low\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc0ebe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_score_ci = rank_awrf.groupby('run_name')['Score'].apply(boot_ci).unstack()\n",
    "run_score_ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9178f47b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_score_full = run_scores.join(run_score_ci)\n",
    "run_score_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caac580",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_tbl_df = run_score_full[['nDCG', 'AWRF', 'Score']].copy()\n",
    "run_tbl_df['95% CI'] = run_score_full.apply(lambda r: \"(%.3f, %.3f)\" % (r['Score.Lo'], r['Score.Hi']), axis=1)\n",
    "run_tbl_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d986bec",
   "metadata": {},
   "source": [
    "Combine them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26f591c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_tbl_fn = Path('figures/task1-runs.tex')\n",
    "run_tbl = run_tbl_df.to_latex(float_format=\"%.4f\", bold_rows=True, index_names=False)\n",
    "run_tbl_fn.write_text(run_tbl)\n",
    "print(run_tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c28120f",
   "metadata": {},
   "source": [
    "## Analyzing Scores\n",
    "\n",
    "What is the distribution of scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fa9fb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_scores.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a789c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.displot(x='Score', data=run_scores)\n",
    "plt.savefig('figures/task1-score-dist.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5438a89e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.relplot(x='nDCG', y='AWRF', hue='team', data=run_scores)\n",
    "sns.rugplot(x='nDCG', y='AWRF', data=run_scores)\n",
    "plt.savefig('figures/task1-ndcg-awrf.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c97d4d",
   "metadata": {},
   "source": [
    "## Per-Topic Stats\n",
    "\n",
    "We need to return per-topic stats to each participant, at least for the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15b0973",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_stats = rank_awrf.groupby('topic_id').agg(['mean', 'median', 'min', 'max'])\n",
    "topic_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dfdc85",
   "metadata": {},
   "source": [
    "Make final score analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d15e16c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_range = topic_stats.loc[:, 'Score']\n",
    "topic_range = topic_range.drop(columns=['mean'])\n",
    "topic_range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16988437",
   "metadata": {},
   "source": [
    "And now we combine scores with these results to return to participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d542791",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ret_dir = Path('results') / 'coordinators'\n",
    "ret_dir.mkdir(exist_ok=True)\n",
    "for system, s_runs in rank_awrf.groupby('run_name'):\n",
    "    aug = s_runs.join(topic_range).reset_index().drop(columns=['run_name'])\n",
    "    fn = ret_dir / f'{system}.tsv'\n",
    "    log.info('writing %s', fn)\n",
    "    aug.to_csv(fn, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1505572-8d3b-4097-a583-8e92fc0a0993",
   "metadata": {},
   "source": [
    "## Individual Dimensions\n",
    "\n",
    "We're now going to process the results on an individual dimension basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858e50f0-16b3-4fca-a7d1-e785bc0fc7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1d_d = {}\n",
    "dim_loop = tqdm(dimensions, desc='dims', leave=False)\n",
    "for dim in dim_loop:\n",
    "    dim_loop.set_postfix_str(dim.name)\n",
    "    t1d = pd.read_parquet(tbl_dir / f'task1-{DATA_MODE}-{dim.name}-target.parquet')\n",
    "    t1d = xr.DataArray(t1d, dims=['topic_id', dim.name])\n",
    "    m1d = metrics.AWRFMetric(qrels.set_index('topic_id'), [dim], t1d)\n",
    "    res1d_d[dim.name] = runs.groupby(['run_name', 'topic_id'])['page_id'].progress_apply(m1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f995f8ad-55a0-48ac-b838-4cc292b4696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1d = pd.concat(res1d_d, names=['dim'])\n",
    "res1d = res1d.unstack().reset_index()\n",
    "res1d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6f649a-7258-4a7d-b249-54434fafc3d4",
   "metadata": {},
   "source": [
    "Now let's group things to get per-dimension metrics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83942b69-6be4-46e9-8c05-759575bd8d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_1d = res1d.groupby(['dim', 'run_name'])['Score'].mean()\n",
    "rr_1d = rr_1d.unstack('dim')\n",
    "rr_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b3c673-ded2-461f-9cd2-93229f1ee132",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "df_1d = run_scores[['Score']].rename(columns={'Score': 'Overall'}).join(rr_1d)\n",
    "df_1d.sort_values('Overall', inplace=True, ascending=False)\n",
    "df_fmt = df_1d.style.highlight_max(props='font-weight: bold')\n",
    "df_fmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88c4f97-5989-4e2d-a507-67cc78406e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1d_fn = Path('figures/task1-single.tex')\n",
    "style = df_1d.style.highlight_max(props='font: bold;').format(lambda x: '%.4f' % (x,))\n",
    "style = style.format_index(axis=0, escape='latex')\n",
    "style = style.hide(names=True)\n",
    "df_tex = style.to_latex()\n",
    "df_1d_fn.write_text(df_tex)\n",
    "print(df_tex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc5c0a1",
   "metadata": {},
   "source": [
    "## Attribute Subset Performance\n",
    "\n",
    "We also want to look at the peformance over *subsets* of the original attributes.  For this, we need two pieces:\n",
    "\n",
    "- The dimensions\n",
    "- The reduced target\n",
    "\n",
    "We'll get the reduced target by marginalizing.  Let's make a function to get dimensions and reduced targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d782a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_dims(dims):\n",
    "    return [d for d in dimensions if d.name in dims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c402a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_tgt(dims):\n",
    "    names = [d.name for d in dimensions if d.name not in dims]\n",
    "    return target.sum(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8da393",
   "metadata": {},
   "source": [
    "### Gender and Geography\n",
    "\n",
    "Last year, we used subject geography and gender.  Let's generate metric results from those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb97234f",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_gender_metric = metrics.AWRFMetric(qrels.set_index('topic_id'), subset_dims(['sub-geo', 'gender']), subset_tgt(['sub-geo', 'gender']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c4f9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_gender_res = runs.groupby(['run_name', 'topic_id'])['page_id'].progress_apply(geo_gender_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdb352e",
   "metadata": {},
   "source": [
    "Now show the results per system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b232eacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_gender_rr = geo_gender_res.unstack().groupby('run_name').mean()\n",
    "geo_gender_rr.sort_values('Score', inplace=True)\n",
    "geo_gender_rr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f33e04",
   "metadata": {},
   "source": [
    "### Internal Properties\n",
    "\n",
    "This year, several of our properties are ‘internal’: that is, they primarily refer to things that matter within the Wikipedia platform, not broader social concerns.\n",
    "\n",
    "Let's see how the systems perform on those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca2b4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_names = ['alpha', 'age', 'pop', 'langs']\n",
    "internal_dims = subset_dims(internal_names)\n",
    "internal_tgt = subset_tgt(internal_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ddf871",
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_metric = metrics.AWRFMetric(qrels.set_index('topic_id'), internal_dims, internal_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fec265",
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_res = runs.groupby(['run_name', 'topic_id'])['page_id'].progress_apply(internal_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af471daf",
   "metadata": {},
   "source": [
    "Now show the results per system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfeba53",
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_rr = internal_res.unstack().groupby('run_name').mean()\n",
    "internal_rr.sort_values('Score', inplace=True)\n",
    "internal_rr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0fbe4a",
   "metadata": {},
   "source": [
    "### Demographic Properties\n",
    "\n",
    "Let's see performance on the other ones (demographic properties):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d08fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_names = [d.name for d in dimensions if d.name not in internal_names]\n",
    "demo_dims = subset_dims(demo_names)\n",
    "demo_tgt = subset_tgt(demo_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87305f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_metric = metrics.AWRFMetric(qrels.set_index('topic_id'), demo_dims, demo_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85db8a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_res = runs.groupby(['run_name', 'topic_id'])['page_id'].progress_apply(demo_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bb460c",
   "metadata": {},
   "source": [
    "Now show the results per system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a573160b",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_rr = demo_res.unstack().groupby('run_name').mean()\n",
    "demo_rr.sort_values('Score', inplace=True)\n",
    "demo_rr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba2d08c",
   "metadata": {},
   "source": [
    "### Subset Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a4200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = {\n",
    "    'Overall': run_scores,\n",
    "    '2021': geo_gender_rr,\n",
    "    'Internal': internal_rr,\n",
    "    'Demographic': demo_rr,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8cefd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_cols = [df[['Score']].rename(columns={'Score': name}) for (name, df) in subsets.items()]\n",
    "ss_df = reduce(lambda df1, df2: df1.join(df2), ss_cols)\n",
    "ss_df.sort_values('Overall', inplace=True, ascending=False)\n",
    "ss_fmt = ss_df.style.highlight_max(props='font-weight: bold;')\n",
    "ss_fmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b22d1b-8b89-40b7-9bf1-f06e95ad819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_fn = Path('figures/task1-subsets.tex')\n",
    "style = ss_df.style.highlight_max(props='font: bold;').format(lambda x: '%.4f' % (x,))\n",
    "style = style.format_index(axis=0, escape='latex')\n",
    "style = style.hide(names=True)\n",
    "ss_tex = style.to_latex()\n",
    "ss_fn.write_text(ss_tex)\n",
    "print(ss_tex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54abe830-efb3-417f-a640-c2ff345b1433",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
